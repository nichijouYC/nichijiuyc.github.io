---
layout: post
title: "机器学习实战笔记(三)"
date: 2015-09-03 22:24:22 +0800
comments: true
categories: 
---
## 分类算法——决策树(decision tree)

### 简单介绍

通过已有的样本数据构建树形分类器对未知样本进行分类
<!-- more -->
### 优缺点

- 优点
    - 计算复杂度不高，易于理解
    - 效率高，决策树只需要一次构建，反复使用
- 缺点
    - 容易发生过拟合问题(指对已有样本数据分类结果较好，但对新数据和与样本数据相差较大的数据分类结果较差)

### 实现步骤(使用ID3算法生成决策树)

1. 计算初始样本的熵
2. 计算按每个特征属性划分数据后获得的信息增益
3. 选择增益最高的特征属性作为划分依据，划分样本
4. 重复1-3步，递归生成整个决策树，直到特征属性全部划分完。  
若特征属性全部划分完后，仍还有剩余样本无法分类，则选择所属类别中出现次数最多的类别作为分支的类别

### 熵(entropy)

- 1948年，香农提出了“信息熵”的概念，解决了对信息的量化度量问题。香农用信息熵的概念来描述信源的不确定度。
- 变量的不确定性越大，熵也就越大，把它搞清楚所需要的信息量也就越大。
- 一个系统越是有序，信息熵就越低；反之，一个系统越是混乱，信息熵就越高。所以，信息熵也可以说是系统有序化程度的一个度量。
- 计算公式：  
![entropy](/pic/entropy.png)

### 信息增益(infoGain)

- 信息增益=infoGain=baseEntropy-newEntropy=原有熵-当前熵
- 特征T给分类F带来的信息增益为IG(T)=H(F)-H(F|T)。
- H(F|T)包含两种情况：一种是特征T出现，标记为t，一种是特征T不出现，标记为t'
- 所以H(F|T)=P(t)H(F|t)+P(t')H(F|t')

### 实现工具包

Python机器学习工具包`sklearn`中的`tree`类
{% codeblock lang:python %}
from sklearn import tree
X = [[1, 1], [1, 1], [1, 0], [0, 1], [0, 1]]
Y = [1, 1, 0, 0, 0]
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)
clf.predict([[1, 1]])
{% endcodeblock %}
