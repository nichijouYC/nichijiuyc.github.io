---
layout: post
title: "机器学习实战笔记(四)"
date: 2015-09-04 19:56:22 +0800
comments: true
categories: 
---
## 分类算法——朴素贝叶斯(NBC)

### 简单介绍

通过贝叶斯定理计算未知样本属于每个类别的概率，选取概率最大的分类作为未知样本的分类  
NBC算法可以同时给出分类猜测结果和它的概率估计值
<!-- more -->
### 优缺点

- 优点
    - 在样本数据较少的情况下也可以处理多类别问题
- 缺点
    - 对输入的数据格式有一定要求

### 贝叶斯理论


- 贝叶斯定理由英国数学家贝叶斯 ( Thomas Bayes 1702-1761 ) 发现，用来描述两个条件概率之间的关系
- 贝叶斯公式如下：  
![bayes](/pic/bayes.png)  
其中，称P(A|B)为后验概率，P(A)为先验概率

### 算法原理和实现步骤

1. 假设样本数据有F1，F2，F3，3个特征属性。样本的类别分为A和B两个类别
2. 对于新样本D来说，若`P(A|D(F1),D(F2),D(F3))` > `P(B|D(F1),D(F2),D(F3))`,那么D属于A类别可能性大。反之，D属于B类别可能性大  
所以只要我们计算出以上两个概率或得知它们的大小即可推断样本D的分类
3. 按贝叶斯公式可得，以上两个概率的大小可以转换成`P(D(F1),D(F2),D(F3)|A)`\*`P(A)`和`P(D(F1),D(F2),D(F3)|B)`\*`P(B)`的大小
4. `P(A)`和`P(B)`可以通过已知样本数据简单得到，下面重点是如何计算`P(D(F1),D(F2),D(F3)|A)`和`P(D(F1),D(F2),D(F3)|B)`
5. 按照朴素贝叶斯理论，假定不同分类之间相互独立(这也是该算法名称的由来)  
那么，`P(D(F1),D(F2),D(F3)|A)`=`P(D(F1)|A)`\*`P(D(F2)|A)`\*`P(D(F3)|A)`
6. `P(D(F1)|A)`，`P(D(F2)|A)`，`P(D(F3)|A)`也可以通过已知样本数据简单得到  
那么我们就可以得到D样本属于A和B的可能性，选择可能性大的那个分类作为D样本的最终分类

注意：在计算第5步中的`P(D(F1)|A)`\*`P(D(F2)|A)`\*`P(D(F3)|A)`时，可能会遇到很多小数相乘溢出的情况。此时，可以转换成对数进行计算,`ln(a*b)=ln(a)+ln(b)`

### 实现工具包

Python机器学习工具包`sklearn`中的`GaussianNB`类
{% codeblock lang:python %}
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
Y = np.array([1, 1, 1, 2, 2, 2])
from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
clf.fit(X, Y)
clf.predict([[-0.8, -1]]))
{% endcodeblock %}
